{"cells":[{"cell_type":"markdown","source":["## CSV"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1dd2eeb-5e3b-4ba0-b583-7a239ddb98e8"}}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType, ArrayType"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1d30f0a-7208-460f-8847-6e5d2a406bc3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_csv(file_path, option_header=False, option_infer_schema=False, option_mode='PERMISSIVE', schema=None):\n  if schema:\n    df = spark.read \\\n              .option('header', option_header) \\\n              .option('inferSchema', option_infer_schema) \\\n              .option('mode', option_mode) \\\n              .schema(schema) \\\n              .csv(file_path)\n  else:\n    df = spark.read \\\n              .option('header', option_header) \\\n              .option('inferSchema', option_infer_schema) \\\n              .option('mode', option_mode) \\\n              .csv(file_path)\n  return df\n\ndf = read_csv('/FileStore/tables/data.csv', option_header=True, option_infer_schema=True)\ndf.show()\ndf.write.csv('FileStore/tables/csv_1', mode='append')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ce00c6b-bc78-4eaa-a079-4e58d51ac661"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------+--------+--------+--------+--------+-------------+-----------------+\n|       Date|    Open|    High|     Low|   Close|Shares Traded|Turnover (Rs. Cr)|\n+-----------+--------+--------+--------+--------+-------------+-----------------+\n|01-Feb-2020| 11939.0|12017.35| 11633.3|11661.85| 5.37634767E8|         20598.12|\n|03-Feb-2020|11627.45|11749.85| 11614.5| 11707.9| 6.69815788E8|         25415.26|\n|04-Feb-2020|11786.25|11986.15| 11783.4|11979.65| 5.60430291E8|          22338.5|\n|05-Feb-2020|12005.85|12098.15|11953.35|12089.15|  7.5803258E8|         22998.05|\n|06-Feb-2020| 12120.0| 12160.6|12084.65|12137.95| 5.65116236E8|         21735.92|\n|07-Feb-2020|12151.15| 12154.7|12073.95|12098.35| 4.73475144E8|         16339.61|\n|10-Feb-2020|12102.35|12103.55|11990.75| 12031.5| 5.25674715E8|         17185.14|\n|11-Feb-2020| 12108.4| 12172.3| 12099.0| 12107.9| 4.80491557E8|         16209.52|\n|12-Feb-2020| 12151.0|12231.75| 12144.3| 12201.2| 4.12399174E8|         16598.33|\n|13-Feb-2020|12219.55|12225.65| 12139.8|12174.65| 5.01510138E8|         16315.27|\n|14-Feb-2020|12190.15| 12246.7| 12091.2|12113.45|  6.2305327E8|         20759.51|\n|17-Feb-2020| 12131.8| 12159.6| 12037.0| 12045.8| 4.55412408E8|         15439.55|\n|18-Feb-2020|12028.25|12030.75|11908.05| 11992.5| 6.77307424E8|          18853.0|\n|19-Feb-2020| 12090.6| 12134.7| 12042.1| 12125.9| 5.14030605E8|         17610.89|\n|20-Feb-2020| 12119.0| 12152.0|12071.45|12080.85| 5.02875583E8|         18831.51|\n|24-Feb-2020|12012.55|12012.55| 11813.4| 11829.4| 4.91224913E8|         19421.04|\n|25-Feb-2020| 11877.5|11883.05| 11779.9| 11797.9| 4.61349973E8|         18510.82|\n|26-Feb-2020|11738.55|11783.25| 11639.6| 11678.5| 5.67990976E8|         21887.07|\n|27-Feb-2020|11661.25|11663.85| 11536.7| 11633.3| 6.09266324E8|         21623.47|\n|28-Feb-2020| 11382.0| 11384.8|11175.05|11201.75| 8.10523106E8|         32297.15|\n+-----------+--------+--------+--------+--------+-------------+-----------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+--------+--------+--------+--------+-------------+-----------------+\n       Date|    Open|    High|     Low|   Close|Shares Traded|Turnover (Rs. Cr)|\n+-----------+--------+--------+--------+--------+-------------+-----------------+\n01-Feb-2020| 11939.0|12017.35| 11633.3|11661.85| 5.37634767E8|         20598.12|\n03-Feb-2020|11627.45|11749.85| 11614.5| 11707.9| 6.69815788E8|         25415.26|\n04-Feb-2020|11786.25|11986.15| 11783.4|11979.65| 5.60430291E8|          22338.5|\n05-Feb-2020|12005.85|12098.15|11953.35|12089.15|  7.5803258E8|         22998.05|\n06-Feb-2020| 12120.0| 12160.6|12084.65|12137.95| 5.65116236E8|         21735.92|\n07-Feb-2020|12151.15| 12154.7|12073.95|12098.35| 4.73475144E8|         16339.61|\n10-Feb-2020|12102.35|12103.55|11990.75| 12031.5| 5.25674715E8|         17185.14|\n11-Feb-2020| 12108.4| 12172.3| 12099.0| 12107.9| 4.80491557E8|         16209.52|\n12-Feb-2020| 12151.0|12231.75| 12144.3| 12201.2| 4.12399174E8|         16598.33|\n13-Feb-2020|12219.55|12225.65| 12139.8|12174.65| 5.01510138E8|         16315.27|\n14-Feb-2020|12190.15| 12246.7| 12091.2|12113.45|  6.2305327E8|         20759.51|\n17-Feb-2020| 12131.8| 12159.6| 12037.0| 12045.8| 4.55412408E8|         15439.55|\n18-Feb-2020|12028.25|12030.75|11908.05| 11992.5| 6.77307424E8|          18853.0|\n19-Feb-2020| 12090.6| 12134.7| 12042.1| 12125.9| 5.14030605E8|         17610.89|\n20-Feb-2020| 12119.0| 12152.0|12071.45|12080.85| 5.02875583E8|         18831.51|\n24-Feb-2020|12012.55|12012.55| 11813.4| 11829.4| 4.91224913E8|         19421.04|\n25-Feb-2020| 11877.5|11883.05| 11779.9| 11797.9| 4.61349973E8|         18510.82|\n26-Feb-2020|11738.55|11783.25| 11639.6| 11678.5| 5.67990976E8|         21887.07|\n27-Feb-2020|11661.25|11663.85| 11536.7| 11633.3| 6.09266324E8|         21623.47|\n28-Feb-2020| 11382.0| 11384.8|11175.05|11201.75| 8.10523106E8|         32297.15|\n+-----------+--------+--------+--------+--------+-------------+-----------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["schema = StructType([\n  StructField('Date', StringType(), True),\n  StructField('Open', DoubleType(), True),\n  StructField('High', DoubleType(), True),\n  StructField('Low', DoubleType(), True),\n  StructField('Close', DoubleType(), True),\n  StructField('SharesTraded', DoubleType(), True),\n  StructField('Turnover', DoubleType(), True),\n  StructField('_corrupt_record', StringType(), True),\n])\n\ndf = read_csv('/FileStore/tables/data.csv', option_header=True, schema = schema)\ndf.show()\ndf.write.partitionBy('Date').csv('/FileStore/tables/csv_2', mode='overwrite')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fa750d5-6947-4304-ba86-784f062bcb05"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------+--------+--------+--------+--------+------------+--------+---------------+\n|       Date|    Open|    High|     Low|   Close|SharesTraded|Turnover|_corrupt_record|\n+-----------+--------+--------+--------+--------+------------+--------+---------------+\n|01-Feb-2020| 11939.0|12017.35| 11633.3|11661.85|5.37634767E8|20598.12|           null|\n|03-Feb-2020|11627.45|11749.85| 11614.5| 11707.9|6.69815788E8|25415.26|           null|\n|04-Feb-2020|11786.25|11986.15| 11783.4|11979.65|5.60430291E8| 22338.5|           null|\n|05-Feb-2020|12005.85|12098.15|11953.35|12089.15| 7.5803258E8|22998.05|           null|\n|06-Feb-2020| 12120.0| 12160.6|12084.65|12137.95|5.65116236E8|21735.92|           null|\n|07-Feb-2020|12151.15| 12154.7|12073.95|12098.35|4.73475144E8|16339.61|           null|\n|10-Feb-2020|12102.35|12103.55|11990.75| 12031.5|5.25674715E8|17185.14|           null|\n|11-Feb-2020| 12108.4| 12172.3| 12099.0| 12107.9|4.80491557E8|16209.52|           null|\n|12-Feb-2020| 12151.0|12231.75| 12144.3| 12201.2|4.12399174E8|16598.33|           null|\n|13-Feb-2020|12219.55|12225.65| 12139.8|12174.65|5.01510138E8|16315.27|           null|\n|14-Feb-2020|12190.15| 12246.7| 12091.2|12113.45| 6.2305327E8|20759.51|           null|\n|17-Feb-2020| 12131.8| 12159.6| 12037.0| 12045.8|4.55412408E8|15439.55|           null|\n|18-Feb-2020|12028.25|12030.75|11908.05| 11992.5|6.77307424E8| 18853.0|           null|\n|19-Feb-2020| 12090.6| 12134.7| 12042.1| 12125.9|5.14030605E8|17610.89|           null|\n|20-Feb-2020| 12119.0| 12152.0|12071.45|12080.85|5.02875583E8|18831.51|           null|\n|24-Feb-2020|12012.55|12012.55| 11813.4| 11829.4|4.91224913E8|19421.04|           null|\n|25-Feb-2020| 11877.5|11883.05| 11779.9| 11797.9|4.61349973E8|18510.82|           null|\n|26-Feb-2020|11738.55|11783.25| 11639.6| 11678.5|5.67990976E8|21887.07|           null|\n|27-Feb-2020|11661.25|11663.85| 11536.7| 11633.3|6.09266324E8|21623.47|           null|\n|28-Feb-2020| 11382.0| 11384.8|11175.05|11201.75|8.10523106E8|32297.15|           null|\n+-----------+--------+--------+--------+--------+------------+--------+---------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+--------+--------+--------+--------+------------+--------+---------------+\n       Date|    Open|    High|     Low|   Close|SharesTraded|Turnover|_corrupt_record|\n+-----------+--------+--------+--------+--------+------------+--------+---------------+\n01-Feb-2020| 11939.0|12017.35| 11633.3|11661.85|5.37634767E8|20598.12|           null|\n03-Feb-2020|11627.45|11749.85| 11614.5| 11707.9|6.69815788E8|25415.26|           null|\n04-Feb-2020|11786.25|11986.15| 11783.4|11979.65|5.60430291E8| 22338.5|           null|\n05-Feb-2020|12005.85|12098.15|11953.35|12089.15| 7.5803258E8|22998.05|           null|\n06-Feb-2020| 12120.0| 12160.6|12084.65|12137.95|5.65116236E8|21735.92|           null|\n07-Feb-2020|12151.15| 12154.7|12073.95|12098.35|4.73475144E8|16339.61|           null|\n10-Feb-2020|12102.35|12103.55|11990.75| 12031.5|5.25674715E8|17185.14|           null|\n11-Feb-2020| 12108.4| 12172.3| 12099.0| 12107.9|4.80491557E8|16209.52|           null|\n12-Feb-2020| 12151.0|12231.75| 12144.3| 12201.2|4.12399174E8|16598.33|           null|\n13-Feb-2020|12219.55|12225.65| 12139.8|12174.65|5.01510138E8|16315.27|           null|\n14-Feb-2020|12190.15| 12246.7| 12091.2|12113.45| 6.2305327E8|20759.51|           null|\n17-Feb-2020| 12131.8| 12159.6| 12037.0| 12045.8|4.55412408E8|15439.55|           null|\n18-Feb-2020|12028.25|12030.75|11908.05| 11992.5|6.77307424E8| 18853.0|           null|\n19-Feb-2020| 12090.6| 12134.7| 12042.1| 12125.9|5.14030605E8|17610.89|           null|\n20-Feb-2020| 12119.0| 12152.0|12071.45|12080.85|5.02875583E8|18831.51|           null|\n24-Feb-2020|12012.55|12012.55| 11813.4| 11829.4|4.91224913E8|19421.04|           null|\n25-Feb-2020| 11877.5|11883.05| 11779.9| 11797.9|4.61349973E8|18510.82|           null|\n26-Feb-2020|11738.55|11783.25| 11639.6| 11678.5|5.67990976E8|21887.07|           null|\n27-Feb-2020|11661.25|11663.85| 11536.7| 11633.3|6.09266324E8|21623.47|           null|\n28-Feb-2020| 11382.0| 11384.8|11175.05|11201.75|8.10523106E8|32297.15|           null|\n+-----------+--------+--------+--------+--------+------------+--------+---------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["error_schema = StructType([\n  StructField('Date', StringType(), True),\n  StructField('Open', DoubleType(), True),\n  StructField('High', DoubleType(), True),\n  StructField('Low', DoubleType(), True),\n  StructField('Close', DoubleType(), True),\n  StructField('SharesTraded', DoubleType(), True),\n  StructField('Turnover', IntegerType(), True),\n  StructField('_corrupt_record', StringType(), True),\n])\n\ndf = read_csv('/FileStore/tables/data.csv', option_header=True, schema = error_schema)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac4ac4af-103a-4538-90bf-26cdd9e91b25"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------+--------+--------+--------+--------+------------+--------+--------------------+\n|       Date|    Open|    High|     Low|   Close|SharesTraded|Turnover|     _corrupt_record|\n+-----------+--------+--------+--------+--------+------------+--------+--------------------+\n|01-Feb-2020| 11939.0|12017.35| 11633.3|11661.85|5.37634767E8|    null|&#34;01-Feb-2020&#34;,&#34;  ...|\n|03-Feb-2020|11627.45|11749.85| 11614.5| 11707.9|6.69815788E8|    null|&#34;03-Feb-2020&#34;,&#34;  ...|\n|04-Feb-2020|11786.25|11986.15| 11783.4|11979.65|5.60430291E8|    null|&#34;04-Feb-2020&#34;,&#34;  ...|\n|05-Feb-2020|12005.85|12098.15|11953.35|12089.15| 7.5803258E8|    null|&#34;05-Feb-2020&#34;,&#34;  ...|\n|06-Feb-2020| 12120.0| 12160.6|12084.65|12137.95|5.65116236E8|    null|&#34;06-Feb-2020&#34;,&#34;  ...|\n|07-Feb-2020|12151.15| 12154.7|12073.95|12098.35|4.73475144E8|    null|&#34;07-Feb-2020&#34;,&#34;  ...|\n|10-Feb-2020|12102.35|12103.55|11990.75| 12031.5|5.25674715E8|    null|&#34;10-Feb-2020&#34;,&#34;  ...|\n|11-Feb-2020| 12108.4| 12172.3| 12099.0| 12107.9|4.80491557E8|    null|&#34;11-Feb-2020&#34;,&#34;  ...|\n|12-Feb-2020| 12151.0|12231.75| 12144.3| 12201.2|4.12399174E8|    null|&#34;12-Feb-2020&#34;,&#34;  ...|\n|13-Feb-2020|12219.55|12225.65| 12139.8|12174.65|5.01510138E8|    null|&#34;13-Feb-2020&#34;,&#34;  ...|\n|14-Feb-2020|12190.15| 12246.7| 12091.2|12113.45| 6.2305327E8|    null|&#34;14-Feb-2020&#34;,&#34;  ...|\n|17-Feb-2020| 12131.8| 12159.6| 12037.0| 12045.8|4.55412408E8|    null|&#34;17-Feb-2020&#34;,&#34;  ...|\n|18-Feb-2020|12028.25|12030.75|11908.05| 11992.5|6.77307424E8|    null|&#34;18-Feb-2020&#34;,&#34;  ...|\n|19-Feb-2020| 12090.6| 12134.7| 12042.1| 12125.9|5.14030605E8|    null|&#34;19-Feb-2020&#34;,&#34;  ...|\n|20-Feb-2020| 12119.0| 12152.0|12071.45|12080.85|5.02875583E8|    null|&#34;20-Feb-2020&#34;,&#34;  ...|\n|24-Feb-2020|12012.55|12012.55| 11813.4| 11829.4|4.91224913E8|    null|&#34;24-Feb-2020&#34;,&#34;  ...|\n|25-Feb-2020| 11877.5|11883.05| 11779.9| 11797.9|4.61349973E8|    null|&#34;25-Feb-2020&#34;,&#34;  ...|\n|26-Feb-2020|11738.55|11783.25| 11639.6| 11678.5|5.67990976E8|    null|&#34;26-Feb-2020&#34;,&#34;  ...|\n|27-Feb-2020|11661.25|11663.85| 11536.7| 11633.3|6.09266324E8|    null|&#34;27-Feb-2020&#34;,&#34;  ...|\n|28-Feb-2020| 11382.0| 11384.8|11175.05|11201.75|8.10523106E8|    null|&#34;28-Feb-2020&#34;,&#34;  ...|\n+-----------+--------+--------+--------+--------+------------+--------+--------------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+--------+--------+--------+--------+------------+--------+--------------------+\n       Date|    Open|    High|     Low|   Close|SharesTraded|Turnover|     _corrupt_record|\n+-----------+--------+--------+--------+--------+------------+--------+--------------------+\n01-Feb-2020| 11939.0|12017.35| 11633.3|11661.85|5.37634767E8|    null|&#34;01-Feb-2020&#34;,&#34;  ...|\n03-Feb-2020|11627.45|11749.85| 11614.5| 11707.9|6.69815788E8|    null|&#34;03-Feb-2020&#34;,&#34;  ...|\n04-Feb-2020|11786.25|11986.15| 11783.4|11979.65|5.60430291E8|    null|&#34;04-Feb-2020&#34;,&#34;  ...|\n05-Feb-2020|12005.85|12098.15|11953.35|12089.15| 7.5803258E8|    null|&#34;05-Feb-2020&#34;,&#34;  ...|\n06-Feb-2020| 12120.0| 12160.6|12084.65|12137.95|5.65116236E8|    null|&#34;06-Feb-2020&#34;,&#34;  ...|\n07-Feb-2020|12151.15| 12154.7|12073.95|12098.35|4.73475144E8|    null|&#34;07-Feb-2020&#34;,&#34;  ...|\n10-Feb-2020|12102.35|12103.55|11990.75| 12031.5|5.25674715E8|    null|&#34;10-Feb-2020&#34;,&#34;  ...|\n11-Feb-2020| 12108.4| 12172.3| 12099.0| 12107.9|4.80491557E8|    null|&#34;11-Feb-2020&#34;,&#34;  ...|\n12-Feb-2020| 12151.0|12231.75| 12144.3| 12201.2|4.12399174E8|    null|&#34;12-Feb-2020&#34;,&#34;  ...|\n13-Feb-2020|12219.55|12225.65| 12139.8|12174.65|5.01510138E8|    null|&#34;13-Feb-2020&#34;,&#34;  ...|\n14-Feb-2020|12190.15| 12246.7| 12091.2|12113.45| 6.2305327E8|    null|&#34;14-Feb-2020&#34;,&#34;  ...|\n17-Feb-2020| 12131.8| 12159.6| 12037.0| 12045.8|4.55412408E8|    null|&#34;17-Feb-2020&#34;,&#34;  ...|\n18-Feb-2020|12028.25|12030.75|11908.05| 11992.5|6.77307424E8|    null|&#34;18-Feb-2020&#34;,&#34;  ...|\n19-Feb-2020| 12090.6| 12134.7| 12042.1| 12125.9|5.14030605E8|    null|&#34;19-Feb-2020&#34;,&#34;  ...|\n20-Feb-2020| 12119.0| 12152.0|12071.45|12080.85|5.02875583E8|    null|&#34;20-Feb-2020&#34;,&#34;  ...|\n24-Feb-2020|12012.55|12012.55| 11813.4| 11829.4|4.91224913E8|    null|&#34;24-Feb-2020&#34;,&#34;  ...|\n25-Feb-2020| 11877.5|11883.05| 11779.9| 11797.9|4.61349973E8|    null|&#34;25-Feb-2020&#34;,&#34;  ...|\n26-Feb-2020|11738.55|11783.25| 11639.6| 11678.5|5.67990976E8|    null|&#34;26-Feb-2020&#34;,&#34;  ...|\n27-Feb-2020|11661.25|11663.85| 11536.7| 11633.3|6.09266324E8|    null|&#34;27-Feb-2020&#34;,&#34;  ...|\n28-Feb-2020| 11382.0| 11384.8|11175.05|11201.75|8.10523106E8|    null|&#34;28-Feb-2020&#34;,&#34;  ...|\n+-----------+--------+--------+--------+--------+------------+--------+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = read_csv('/FileStore/tables/data.csv', option_header=True, schema = error_schema, option_mode='FAILFAST')\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"291a98c8-6a6a-45fe-a931-29b10dcc6b23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3860275317133770&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> df <span class=\"ansi-blue-fg\">=</span> read_csv<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/FileStore/tables/data.csv&#39;</span><span class=\"ansi-blue-fg\">,</span> option_header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> error_schema<span class=\"ansi-blue-fg\">,</span> option_mode<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;FAILFAST&#39;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    488</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    489</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 490</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    491</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    492</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o656.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 17) (ip-10-172-242-196.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/data.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:389)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:368)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:459)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:300)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:754)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:178)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option &#39;mode&#39; as &#39;PERMISSIVE&#39;.\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:96)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:419)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:333)\n\t... 18 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: &#34;        20598.12&#34;\nCaused by: java.lang.NumberFormatException: For input string: &#34;        20598.12&#34;\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:569)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:162)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:162)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:243)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:162)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:296)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:259)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:411)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:83)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:419)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:333)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:459)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:300)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:754)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:178)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2766)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2713)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2707)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2707)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1256)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2974)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2915)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2903)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2458)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:289)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:496)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:495)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:399)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:58)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3018)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3797)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3789)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3787)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2949)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:306)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:343)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 17) (ip-10-172-242-196.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/data.csv.","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3860275317133770&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> df <span class=\"ansi-blue-fg\">=</span> read_csv<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/FileStore/tables/data.csv&#39;</span><span class=\"ansi-blue-fg\">,</span> option_header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> error_schema<span class=\"ansi-blue-fg\">,</span> option_mode<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;FAILFAST&#39;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    488</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    489</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 490</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    491</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    492</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o656.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 17) (ip-10-172-242-196.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/data.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:389)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:368)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:459)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:300)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:754)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:178)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option &#39;mode&#39; as &#39;PERMISSIVE&#39;.\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:96)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:419)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:333)\n\t... 18 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: &#34;        20598.12&#34;\nCaused by: java.lang.NumberFormatException: For input string: &#34;        20598.12&#34;\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:569)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:162)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:162)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:243)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:162)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:296)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:259)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:411)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:83)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:419)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:333)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:459)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:300)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:754)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:178)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2766)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2713)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2707)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2707)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1256)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2974)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2915)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2903)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2458)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:289)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:496)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:495)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:399)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:58)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3018)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3797)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3789)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3787)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2949)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:306)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:343)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = read_csv('/FileStore/tables/data.csv', option_header=True, schema = error_schema, option_mode='DROPMALFORMED')\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"170c715e-0ef2-4a77-b1c3-19b950604d09"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+----+----+---+-----+------------+--------+---------------+\n|Date|Open|High|Low|Close|SharesTraded|Turnover|_corrupt_record|\n+----+----+----+---+-----+------------+--------+---------------+\n+----+----+----+---+-----+------------+--------+---------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+---+-----+------------+--------+---------------+\nDate|Open|High|Low|Close|SharesTraded|Turnover|_corrupt_record|\n+----+----+----+---+-----+------------+--------+---------------+\n+----+----+----+---+-----+------------+--------+---------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## JSON"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48ac6da6-c9cf-42a8-a9bb-305e0c8d4b25"}}},{"cell_type":"code","source":["df = spark.read.option('multiline', True).json('/FileStore/tables/sample-1.json')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c34b2b23-171a-4311-a9a1-bc9da5eac47a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0eb5835e-6dbc-49e2-b238-88fbb26509ec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------------------------------------------------------------------------------+----+----+----+-----------------------------------------------------------------------------------------------------------------------------------------+-----+\n|batters                                                                        |id  |name|ppu |topping                                                                                                                                  |type |\n+-------------------------------------------------------------------------------+----+----+----+-----------------------------------------------------------------------------------------------------------------------------------------+-----+\n|{[{1001, Regular}, {1002, Chocolate}, {1003, Blueberry}, {1004, Devil&#39;s Food}]}|0001|Cake|0.55|[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|donut|\n+-------------------------------------------------------------------------------+----+----+----+-----------------------------------------------------------------------------------------------------------------------------------------+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------------------------------------------------------------+----+----+----+-----------------------------------------------------------------------------------------------------------------------------------------+-----+\nbatters                                                                        |id  |name|ppu |topping                                                                                                                                  |type |\n+-------------------------------------------------------------------------------+----+----+----+-----------------------------------------------------------------------------------------------------------------------------------------+-----+\n{[{1001, Regular}, {1002, Chocolate}, {1003, Blueberry}, {1004, Devil&#39;s Food}]}|0001|Cake|0.55|[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|donut|\n+-------------------------------------------------------------------------------+----+----+----+-----------------------------------------------------------------------------------------------------------------------------------------+-----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["schema = StructType([\n  StructField('id', StringType()),\n  StructField('name', StringType()),\n  StructField('ppu', DoubleType()),\n  StructField('batters', StructType([\n    StructField('batter', ArrayType(\n      StructType([\n        StructField('id', StringType()),\n        StructField('type', StringType())\n      ])\n    ))\n  ])),\n  StructField('topping', ArrayType(\n    StructType([\n      StructField('id', StringType()),\n      StructField('type', StringType())\n    ])\n  ))\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5c32c62-c5fb-437b-b021-2ce60f7c25d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.option('multiline', True).schema(schema).json('/FileStore/tables/sample-1.json')\ndf.write.partitionBy('name').bucketBy(4, 'ppu').json('/FileStore/tables/json_1')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81e803e5-dc17-4806-a33e-70ad485b46cd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d7314f4-ed77-4f37-971f-6dce98745a9c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+----+----+-------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n|id  |name|ppu |batters                                                                        |topping                                                                                                                                  |\n+----+----+----+-------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n|0001|Cake|0.55|{[{1001, Regular}, {1002, Chocolate}, {1003, Blueberry}, {1004, Devil&#39;s Food}]}|[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|\n+----+----+----+-------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+-------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\nid  |name|ppu |batters                                                                        |topping                                                                                                                                  |\n+----+----+----+-------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n0001|Cake|0.55|{[{1001, Regular}, {1002, Chocolate}, {1003, Blueberry}, {1004, Devil&#39;s Food}]}|[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|\n+----+----+----+-------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## PARQUET"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7d87595-aac3-4129-b64f-c38ea1c8784e"}}},{"cell_type":"code","source":["df = spark.read.option('mergeSchema', True).parquet('/FileStore/tables/par2')\ndf.reparition(4).write.parquet('/FileStore/tables/parquet_1', mode='ignore')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3afc8c91-6572-4a08-b499-10284c485bcf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc2c955e-a822-42dc-916a-ca0a19cde75e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------+---+\n|                name| id|\n+--------------------+---+\n|&lt;Name firstname=&#34;...|  1|\n|&lt;Name firstname=&#34;...|  2|\n+--------------------+---+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---+\n                name| id|\n+--------------------+---+\n&lt;Name firstname=&#34;...|  1|\n&lt;Name firstname=&#34;...|  2|\n+--------------------+---+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## JDBC"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7064ef7b-be9b-47e4-80d4-ac4384267354"}}},{"cell_type":"code","source":["jdbcUrl = \"jdbc:postgresql:dbserver\"\ndbTable = \"employees\"\nconnectionProperties = {\"user\": \"username\", \"password\": \"password\"}\ndf = spark.read.jdbc(jdbcUrl, dbTable, connectionProperties)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81e16b87-c234-4053-b305-18f0e19f5406"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2 Read_Write Data","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2405976335296843}},"nbformat":4,"nbformat_minor":0}
